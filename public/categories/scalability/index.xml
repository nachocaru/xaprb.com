<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Scalability on Xaprb </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://www.xaprb.com/categories/scalability/index.xml/</link>
    
    
    
    <updated>Mon, 07 Jan 2013 00:00:00 UTC</updated>
    
    <item>
      <title>A close look at New Relic&#39;s scalability chart</title>
      <link>http://www.xaprb.com/blog/2013/01/07/a-close-look-at-new-relics-scalability-chart/</link>
      <pubDate>Mon, 07 Jan 2013 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2013/01/07/a-close-look-at-new-relics-scalability-chart/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve written a lot about modeling MySQL with the USL, and I like it best of all the scalability models I&amp;rsquo;ve seen, but it&amp;rsquo;s not the only way to think about scalability. I was aware that New Relic supports a scalability chart, so I decided to take a peek at that. Here&amp;rsquo;s a screenshot of the chart, from &lt;a href=&#34;http:/http://www.xaprb.com/blog.newrelic.com/2011/06/13/of-rainbows-and-polka-dots-new-relics-scalability-charts-explained/&#34;&gt;their blog&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img alt=&#34;blog-rpm-response1&#34; src=&#34;http://www.xaprb.com/media/2013/01/blog-rpm-response1.png&#34; width=&#34;510&#34; height=&#34;295&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how it works. It plots response time (or database time, or CPU) as the dependent variable, versus throughput as the independent variable. There&amp;rsquo;s a line through it to indicate the general shape. Samples are charted as points in a scatter plot. The points are color-coded by the time of day. Outliers are automatically removed.&lt;/p&gt;

&lt;p&gt;The focus on response time is really good. That&amp;rsquo;s one of the things I like about New Relic. While most systems show people status counters, and imply that they have some deep insight and meaningfulness (there&amp;rsquo;s usually no meaning to be found in status counters!), New Relic is educating people about the importance of response time, or latency.&lt;/p&gt;

&lt;p&gt;But as I read through the blog posts about this chart, it struck me that there&amp;rsquo;s something a little odd about it. The problem, I realized, is that it plots throughput as the independent variable on the chart. But throughput isn&amp;rsquo;t an independent variable. Throughput is the system&amp;rsquo;s output under load, and depends on a) the load on the system, b) the system&amp;rsquo;s scalability. It&amp;rsquo;s a &lt;em&gt;dependent&lt;/em&gt; variable.&lt;/p&gt;

&lt;p&gt;In a chart like this, it would be even better to show the independent variable as the variable that one can really control: the concurrency or load on the system. By &amp;ldquo;load&amp;rdquo; I mean the usual definition: the amount of work waiting to be completed, i.e. the backlog; this is what a Unix load average measures.&lt;/p&gt;

&lt;p&gt;To explain a little more what I mean about throughput being dependent, not independent, here are a few ways to think about it:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An independent variable should range from zero to infinity (negative numbers are unphysical in a situation like this, so we exclude that). Throughput has a very finite theoretical and practical upper bound, but concurrency can theoretically go to infinity as work arrives and doesn&amp;rsquo;t complete.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An independent variable is the variable &lt;em&gt;you can control as an input parameter of a system under test&lt;/em&gt;. It&amp;rsquo;s dead-easy to achieve the desired concurrency for a benchmark or other test. It&amp;rsquo;s &lt;em&gt;amazingly&lt;/em&gt; difficult to manufacture a desired throughput for a benchmark, even in &amp;ldquo;easy&amp;rdquo; conditions. Computers are unruly beasts &amp;ndash; they are queueing systems, and random variations and dependencies cause throughput to fluctuate greatly. That&amp;rsquo;s because throughput is measured at the &lt;em&gt;output&lt;/em&gt; end of the system, after the queues inside the system have had their way with the input and introduced statistical fluctuations into it. It&amp;rsquo;s quite easy to generate a desired &lt;em&gt;arrival rate&lt;/em&gt; for a system under test, provided that you have an unbounded number of workers ready to keep submitting more requests as the system queues up and stalls existing workers, but arrivals are not the same as throughput :-) Any way you look at it, you can pick your concurrency and your arrival rate, but you really can&amp;rsquo;t pick your throughput reliably. Throughput is an effect, not a cause.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An independent variable in a function must map to one and only one value of the dependent variable. But we know that as load increases, a system&amp;rsquo;s throughput rises, peaks, and then falls again as retrograde scalability manifests itself. Suppose a system&amp;rsquo;s throughput goes from 10,000 queries per second at 16 threads, to 20,000 at 32 threads, and back to 10,000 at 64 threads. Now if we flip the chart&amp;rsquo;s axes around and treat throughput as an input, we&amp;rsquo;ll find that a throughput of 10,000 queries per second would map to either 16 or 64 threads. That doesn&amp;rsquo;t describe a real function.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So although the New Relic scalability chart shows some of the &lt;em&gt;effects&lt;/em&gt; of the system&amp;rsquo;s scalability, and it&amp;rsquo;s great to visualize the variation in response time as throughput varies, it doesn&amp;rsquo;t strike me as quite the right angle of approach.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m curious to hear from people who may have used this feature. What did you use it for? Were you successful in gaining insight into scalability bottlenecks? How did it help you?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling scalability with the USL at concurrencies less than 1</title>
      <link>http://www.xaprb.com/blog/2013/01/05/modeling-scalability-with-the-usl-at-concurrencies-less-than-1/</link>
      <pubDate>Sat, 05 Jan 2013 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2013/01/05/modeling-scalability-with-the-usl-at-concurrencies-less-than-1/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/&#34; title=&#34;Determining the USL’s coefficient of performance, part 2&#34;&gt;Last time&lt;/a&gt; I said that you can set a starting value for the USL&amp;rsquo;s coefficient of performance and let your modeling software (R, gnuplot, etc) manipulate this as part of the regression to find the best fit. However, there is a subtlety in the USL model that you need to be aware of. Here is a picture of the low-end of the curve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.xaprb.com/media/2013/01/usl1.png&#34; alt=&#34;usl&#34; width=&#34;490&#34; height=&#34;486&#34; class=&#34;aligncenter size-full wp-image-3008&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The graph shows the USL model as the blue curve and linear scalability as the black line. Notice that at concurrencies less than 1, the value of the USL function is actually greater than the linear scalability function. This deserves some thought and explanation, because it can cause problems.&lt;/p&gt;

&lt;p&gt;If you think about it, concurrency between one and zero is impossible. In fact, concurrency is not a smooth function, it is a step function. There can be zero requests resident in the system, one request, two requests, and so on &amp;ndash; but not 0.7 requests or 3.14159 requests. However, the USL is defined in terms of a continuous function, not a step function.&lt;/p&gt;

&lt;p&gt;The trouble with the MySQL systems I usually model is that I generally observe them in the wild, which means that I get a large number of samples of throughput-and-concurrency, and I aggregate them. For example, I&amp;rsquo;ll usually observe concurrency once per second, and average these samples over a minute or more for each point I want to feed into the USL model. This approach generates concurrency values that are real numbers, not just integers &amp;ndash; so it&amp;rsquo;s entirely possible that during a given minute, the &amp;ldquo;average concurrency&amp;rdquo; on the system comes out to 0.7 or 3.14159. What&amp;rsquo;s to be done with this?&lt;/p&gt;

&lt;p&gt;In the perfect world, I&amp;rsquo;d like to delete &amp;ldquo;empty space&amp;rdquo; during which zero queries were executing, and determine the actual throughput at each integral value of concurrency. But it&amp;rsquo;s a lot less convenient to do this, at best; and it&amp;rsquo;s usually impractical or impossible. So I work with the data I have. In practice I find it&amp;rsquo;s good enough.&lt;/p&gt;

&lt;p&gt;Back to the funny anomaly where the USL predicts better-than-linear scalability between concurrency=0 and =1. The outcome is that the regression to the USL model can potentially skew the values of the USL coefficients, if you have any samples that lie between 0 and 1. Thus, it may be a good idea to discard these samples. This should not be a significant portion of your sample dataset anyway. If you don&amp;rsquo;t have a lot of samples at higher concurrencies, you probably don&amp;rsquo;t have enough data to model the system accurately, and you should act accordingly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Determining the USL&#39;s coefficient of performance, part 2</title>
      <link>http://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/</link>
      <pubDate>Thu, 03 Jan 2013 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2013/01/03/determining-the-usls-coefficient-of-performance-part-2/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.xaprb.com/blog/2013/01/02/determining-the-universal-scalability-laws-coefficient-of-performance/&#34; title=&#34;Determining the Universal Scalability Law’s coefficient of performance&#34;&gt;Last time&lt;/a&gt; I said that the USL has a forgotten third coefficient, the coefficient of performance. This is the same thing as the system&amp;rsquo;s throughput at concurrency=1, or C(1). How do you determine this coefficient? There are at least three ways.&lt;/p&gt;

&lt;p&gt;Neil Gunther&amp;rsquo;s writings, or at least those that I&amp;rsquo;ve read and remember, say that you should set it equal to your measurement of C(1). Most of his writing discusses a handful of measurements of the system: one at concurrency 1, and at least 4 to 6 at higher concurrencies. I can&amp;rsquo;t remember a time when he&amp;rsquo;s discussed taking more than one measurement of throughput at each level of concurrency, so I think the assumption is that you&amp;rsquo;re going to take a single measurement at various concurrencies (or, in the case of hardware scalability, units of hardware), and you&amp;rsquo;re done.&lt;/p&gt;

&lt;p&gt;This tends to work quite well. I&amp;rsquo;ve blogged before about this: well-designed systems, measured in a carefully controlled test, tend to match the Universal Scalability Law model quite well. Here are &lt;a href=&#34;http://www.mysqlperformanceblog.com/2011/01/26/modeling-innodb-scalability-on-multi-core-servers/&#34;&gt;two&lt;/a&gt; &lt;a href=&#34;http://www.mysqlperformanceblog.com/2011/02/28/is-voltdb-really-as-scalable-as-they-claim/&#34;&gt;examples&lt;/a&gt;.
Most systems I model aren&amp;rsquo;t like that. I don&amp;rsquo;t do my modeling in a lab. I get thousands, if not tens or hundreds of thousands, of measurements of throughput and concurrency from a MySQL server&amp;rsquo;s real production traffic. How do you determine the system&amp;rsquo;s throughput at concurrency=1 in this kind of situation? You may have hundreds or thousands of samples at or near concurrency=1, and here&amp;rsquo;s the interesting thing: they aren&amp;rsquo;t tightly clustered. This leads to the two additional techniques I&amp;rsquo;ve used.&lt;/p&gt;

&lt;p&gt;Method 2 is fairly obvious: you can take an aggregate measure of the throughput at N=1. You can simply average, or you can use the median. In my experience, the latter tends to be a little more accurate, because the median essentially discards outliers. Given enough samples, it is very likely that the median is truly representative of the system&amp;rsquo;s real behavior.&lt;/p&gt;

&lt;p&gt;Finally, method 3 is to treat C(1) as one of the parameters to fit in the regression to the USL model. Instead of holding it as a fixed quantity, go ahead and let the regression find the best fit for it along with the other coefficients.&lt;/p&gt;

&lt;p&gt;In practice, I tend to combine methods 2 and 3. I use method 2 to find a starting point for the coefficient, and then I let the regression tweak it as needed. In my experience, this usually produces good results. Sometimes the software doing the regression gets a little confused, or stuck at a local maximum, but otherwise it works well.&lt;/p&gt;

&lt;p&gt;What if you don&amp;rsquo;t have measurements at N=1? The best approach, in my experience, is to take the slope of the line from the first data point you have, and use that. N=1 will almost always be higher than this, because real systems are rarely linearly scalable. That&amp;rsquo;s okay. If you let the regression adjust the coefficient as needed for the best fit, you&amp;rsquo;ll end up with a good answer anyway.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Determining the Universal Scalability Law&#39;s coefficient of performance</title>
      <link>http://www.xaprb.com/blog/2013/01/02/determining-the-universal-scalability-laws-coefficient-of-performance/</link>
      <pubDate>Wed, 02 Jan 2013 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2013/01/02/determining-the-universal-scalability-laws-coefficient-of-performance/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re familiar with Neil Gunther&amp;rsquo;s Universal Scalability Law, you may have heard it said that there are two coefficients, variously called alpha and beta or sigma and kappa. There are actually three coefficients, though. See?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.xaprb.com/media/2013/01/usl.png&#34; alt=&#34;usl&#34; width=&#34;637&#34; height=&#34;188&#34; class=&#34;aligncenter size-full wp-image-3000&#34; /&gt;&lt;/p&gt;

&lt;p&gt;No, you don&amp;rsquo;t see it &amp;ndash; but it&amp;rsquo;s actually there, as a hidden &amp;ldquo;1&amp;#8243; multiplied by N in the numerator on the right-hand side. When you&amp;rsquo;re using the USL to model a system&amp;rsquo;s scalability, you need to use the C(1), the &amp;ldquo;capacity at one,&amp;rdquo; as a multiplier. I call this the coefficient of performance. It&amp;rsquo;s rarely 1; it&amp;rsquo;s usually thousands.&lt;/p&gt;

&lt;p&gt;To illustrate why this matters, consider two systems&amp;rsquo; throughput as load increases:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.xaprb.com/media/2013/01/coeff-of-performance.png&#34; alt=&#34;coeff-of-performance&#34; width=&#34;489&#34; height=&#34;486&#34; class=&#34;aligncenter size-full wp-image-3001&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The green line and the blue line are both linearly scalable systems. Add twice the concurrency, get twice the throughput. But the slope of the lines is different. The green system can do three times as much work as the blue system, even though it&amp;rsquo;s no more scalable.&lt;/p&gt;

&lt;p&gt;To model the USL, you need to determine C(1) by measuring the system under test. In my experience with real systems running in production, mostly MySQL servers, this is not simple. You can&amp;rsquo;t just say &amp;ldquo;let&amp;rsquo;s quiet the web app down, I want to load it with exactly one user for a few minutes and measure how fast it runs.&amp;rdquo; Instead, you get a bunch of samples from production traffic, and you derive the throughput at concurrency=1 from that.&lt;/p&gt;

&lt;p&gt;The result goes into the numerator as a multiplier of N, although it&amp;rsquo;s usually omitted when the USL formula is shown.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scalability, performance, capacity planning and USL at Hotsos Symposium</title>
      <link>http://www.xaprb.com/blog/2012/03/07/scalability-performance-capacity-planning-and-usl-at-hotsos-symposium/</link>
      <pubDate>Wed, 07 Mar 2012 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2012/03/07/scalability-performance-capacity-planning-and-usl-at-hotsos-symposium/</guid>
      <description>&lt;p&gt;I presented at this year&amp;rsquo;s [Hotsos Symposium][1]. I am searching for a claim to specialness, and I think it may be that I am the first Hotsos presenter who&amp;rsquo;s specifically focused on MySQL. True? I don&amp;rsquo;t know, but I&amp;rsquo;ll run with it for now.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://vividcortex.com/&#34;&gt;VividCortex&lt;/a&gt; is the startup I founded in 2012. It&amp;rsquo;s the easiest way to monitor what
your servers are doing in production. It does TCP network
traffic analysis. VividCortex offers &lt;a href=&#34;https://vividcortex.com/monitoring/mysql/&#34;&gt;MySQL performance
monitoring&lt;/a&gt; and &lt;a href=&#34;https://vividcortex.com/monitoring/postgres/&#34;&gt;PostgreSQL
performance management&lt;/a&gt; among many
other features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My topic was on extracting black-box performance metrics from TCP packet headers and timestamps and finding hidden performance problems in the system, without any knowledge of what the client and server are talking to each other about. I then extended the same data to performance and scalability modeling, which you can use for purposes such as forecasting, capacity planning, and bottleneck analysis.&lt;/p&gt;

&lt;p&gt;This technique works on MySQL because its TCP protocol is half-duplex, and it&amp;rsquo;ll work for any system with a half-duplex protocol. Does it work on Oracle Database? I am not sure, and no one else I&amp;rsquo;ve spoken to yet has been certain either. I can probably find out with a little research into the Oracle Database protocol.&lt;/p&gt;

&lt;p&gt;I wrote a white paper that goes into my presentation topics in more details. You can find it [here][2], along with sample data and commands that you can use to reproduce my results. This covers Part I of my presentation, and I will publish another white paper with Part II in a while; probably after the [MySQL conference][3] in April.&lt;/p&gt;

&lt;p&gt;My techniques are based on models and approaches that [Neil Gunther][4] developed, and Neil himself presented just after I did. His talk was about power-law distributions, and how a log-log plot renders a power-law relationship as linear. It turns out that power laws are related to fractals, the coastline of Britain, the frequency of word usage in the English language, and [response time in Oracle workloads][5]. I&amp;rsquo;m sure he will post some details on his [blog][6].&lt;/p&gt;

&lt;p&gt;After the day&amp;rsquo;s sessions ended, I ended up talking to Neil for a while. He explained and clarified a lot of things I didn&amp;rsquo;t understand about his work, such as the relationship between repairman queueing and the Universal Scalability Law. He also saw through a variety of my misconceptions and set me straight. Apparently I need to attend one of his training classes. We followed this by eating dinner and sharing a bottle of wine until late in the evening, and Neil wouldn&amp;rsquo;t let me pay in the end. A most enjoyable meal and conversation. Next time it&amp;rsquo;s on me!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fundamental performance and scalability instrumentation</title>
      <link>http://www.xaprb.com/blog/2011/10/06/fundamental-performance-and-scalability-instrumentation/</link>
      <pubDate>Thu, 06 Oct 2011 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2011/10/06/fundamental-performance-and-scalability-instrumentation/</guid>
      <description>&lt;p&gt;This post is a followup to some promises I made at Postgres Open.&lt;/p&gt;

&lt;p&gt;Instrumentation can be a lot of work to add to a server, and it can add overhead to the server too. The bits of instrumentation I&amp;rsquo;ll advocate in this post are few and trivial, but disproportionately powerful.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: &lt;a href=&#34;https://vividcortex.com/&#34;&gt;VividCortex&lt;/a&gt; is the startup I founded in 2012. It&amp;rsquo;s the easiest way to monitor what
your servers are doing in production. VividCortex offers &lt;a href=&#34;https://vividcortex.com/monitoring/mysql/&#34;&gt;MySQL performance
monitoring&lt;/a&gt; and &lt;a href=&#34;https://vividcortex.com/monitoring/postgres/&#34;&gt;PostgreSQL
performance management&lt;/a&gt; among many
other features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If all server software shipped with these metrics as the basic starting point, it would change the world forever:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Time elapsed, in high resolution (preferably microseconds; milliseconds is okay; one-second is mostly useless). When I ask for this counter, it simply tells me either the time of day, or the server&amp;rsquo;s uptime, or something like that. It can be used to determine the boundaries of an observation interval, defined by two measurements. It needs to be consistent with the other metrics that I&amp;rsquo;ll explain next.&lt;/li&gt;
&lt;li&gt;The number of queries (statements) that have completed.&lt;/li&gt;
&lt;li&gt;The current number of queries being executed.&lt;/li&gt;
&lt;li&gt;The total execution time of all queries, including the in-progress time of currently executing queries, in high resolution. That is, if two queries executed with 1 second of response time each, the result is 2 seconds, no matter whether the queries executed concurrently or serially. If one query started executing .5 seconds ago and is still executing, it should contribute .5 second to the counter.&lt;/li&gt;
&lt;li&gt;The server&amp;rsquo;s total busy time, in high resolution. This is different from the previous point in that it only shows the portion of the observation interval during which queries were executing, regardless of whether they were concurrent or not. If two queries with 1-second response time executed serially, the counter is 2. If they executed concurrently, the counter is something less than 2, because the overlapping time isn&amp;rsquo;t double-counted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In practice, these can be maintained as follows, in pseudo-code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;global timestamp;
global concurrency;
global busytime;
global totaltime;
global queries;

function run_query() {
  local now = time();
  if ( concurrency ) {
    busytime += now - timestamp;
    totaltime += (now - timestamp) * concurrency;
  }
  concurrency++;
  timestamp = now;

  // Execute the query, and when it completes...

  now = time();
  busytime += now - timestamp;
  totaltime += (now - timestamp) * concurrency;
  concurrency--;
  timestamp = now;
  queries++;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I may have missed something there; I&amp;rsquo;m writing this off the cuff. If I&amp;rsquo;ve messed up, let me know and I&amp;rsquo;ll fix it. In any case, these metrics can be used to derive all sorts of powerful things through applications of Little&amp;rsquo;s Law and queueing theory, as well as providing the inputs to the Universal Scalability Law. They should be reported by simply reading from the variables marked as &amp;ldquo;global&amp;rdquo; above, to provide a consistent view of the metrics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using BASE instead of ACID for scalability</title>
      <link>http://www.xaprb.com/blog/2008/07/23/using-base-instead-of-acid-for-scalability/</link>
      <pubDate>Wed, 23 Jul 2008 00:00:00 UTC</pubDate>
      
      <guid>http://www.xaprb.com/blog/2008/07/23/using-base-instead-of-acid-for-scalability/</guid>
      <description>&lt;p&gt;My editor &lt;a href=&#34;http://www.oreillynet.com/pub/au/36&#34;&gt;Andy Oram&lt;/a&gt; recently sent me an &lt;a href=&#34;http://acmqueue.com/modules.php?name=Content&amp;amp;#038;pa=showpage&amp;amp;#038;pid=540&#34;&gt;ACM article on BASE, a technique for improving scalability&lt;/a&gt; by being willing to give up some other properties of traditional transactional systems.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a really good read. In many ways it is the same religion everyone who&amp;rsquo;s successfully scaled a system Really Really Big has advocated. But this is different: it&amp;rsquo;s a very clear article, with a great writing style that really cuts out the fat and teaches the principles without being specific to any environment or sounding egotistical.&lt;/p&gt;

&lt;p&gt;He mentions a lot of current thinking in the field, including the CAP principle, which &lt;a href=&#34;http://www.continuent.com/&#34;&gt;Robert Hodges of Continuent&lt;/a&gt; first turned me onto a couple months ago. &lt;a href=&#34;http://citeseer.ist.psu.edu/544596.html&#34;&gt;It has been proven formally&lt;/a&gt;, though I have not read the proof myself.&lt;/p&gt;

&lt;p&gt;One of the most important concepts he advances is giving up the illusion of control. As programmers and DBAs, I think we may tend to like control too much. Foreign keys are a perfect example. I think the point here is that these things make you feel safe, but they don&amp;rsquo;t really make you safe. Just as with so many things in life, recognizing our inability to really control the systems we build is key to working with their strengths &amp;ndash; instead of trying to bind them with iron bands.&lt;/p&gt;

&lt;p&gt;Another great point is idempotency. This is a great way to help avoid problems with MySQL replication, by the way. I&amp;rsquo;ll leave the &amp;ldquo;why&amp;rdquo; as an exercise for the reader, but let me just point out that the file MySQL uses to remember its current position in replication is not synced to disk, so it will almost certainly get out of whack if MySQL dies ungracefully. (Google has solved this problem.)&lt;/p&gt;

&lt;p&gt;A highly recommended read &amp;ndash; worth more than most case studies about how specific companies have scaled their specific systems.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>