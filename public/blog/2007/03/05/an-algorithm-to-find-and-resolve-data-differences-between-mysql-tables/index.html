<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> An algorithm to find and resolve data differences between MySQL tables &middot; Xaprb </title>

  
  <link rel="stylesheet" href="http://www.xaprb.com/css/poole.css">
  <link rel="stylesheet" href="http://www.xaprb.com/css/syntax.css">
  <link rel="stylesheet" href="http://www.xaprb.com/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Xaprb" />
</head>

<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>Xaprb</h1>
      <p class="lead">
		Stay Curious!
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
        <li><a href="http://www.xaprb.com/about/"> About </a></li>
      
        <li><a href="http://www.xaprb.com/essential-books/"> Essential Books </a></li>
      
        <li><a href="http://www.xaprb.com/rx-toolkit/"> Regex Toolkit </a></li>
      
    </ul>

	 <p>
	 &copy; Baron Schwartz.
	 Contact: moc.brpax@norab, <a href="https://twitter.com/xaprb">Twitter</a>, <a href="https://linkedin.com/in/xaprb">LinkedIn</a>.
	 <br>Powered by <a href="http://hugo.spf13.com">Hugo</a>. <a href="/index.xml">RSS Feed</a></p>
						            
  </div>
</div>


    <div class="content container">
<div class="post">
  <h1>An algorithm to find and resolve data differences between MySQL tables</h1>
  <span class="post-date">Mon, Mar 5, 2007 in
		
		<a href="/categories/Databases" class="btn btn-primary">Databases</a>
		
  </span>
      

<p>I&rsquo;ve been designing an algorithm to resolve data differences between MySQL
tables, specifically so I can &lsquo;patch&rsquo; a replication slave that has gotten
slightly out of sync without completely re-initializing it. I intend to create
a tool that can identify which rows are different and bring them into sync. I
would like your thoughts on this.</p>

<h3 id="toc_0">Background and requirements</h3>

<p>I see this as the next step in my recent series of posts on MySQL tools and
techniques to keep replication running reliably and smoothly. Sometimes slaves
&ldquo;drift&rdquo; a little bit, even when there don&rsquo;t seem to be any issues with
replication (this is one reason I submitted a bug report to add <a href="http://bugs.mysql.com/bug.php?id=25737">checksums on
binlog events</a>).
Once a table differs on the slave, it gets more and more different from the
master, possibly causing other tables to differ too.</p>

<p>I need a tool that, given a table known to differ on master and slave(s), will
<em>efficiently</em> compare the tables and resolve the differences. Finding tables
that differ is easy with <a href="http://sourceforge.net/projects/mysqltoolkit">MySQL Table
Checksum</a>,
but I am not sure the best way to find which rows differ.</p>

<p>Here are my requirements. The algorithm needs to be:</p>

<ul>
<li>Designed for statement-based replication, which means no temp tables, no expensive queries that will propagate to the slave, and so forth.</li>
<li>Efficient in terms of network load and server load, both when finding and when resolving differences. No huge tables or un-indexed data, no high-impact <code>INSERT.. SELECT</code> locking, etc.</li>
<li>Efficient on the client-side where the tool is executed.</li>
<li>Must work well on &ldquo;very large&rdquo; tables.</li>
</ul>

<p>Some things I assume:</p>

<ul>
<li>Tables must have primary keys. Without primary keys, it&rsquo;s hard or a waste of time at best, and a disaster at worst.</li>
<li>It is not a good idea to do this unless the fraction of rows that differ is very small. If much of the table is different, then <code>mysqldump</code> is a better idea.</li>
</ul>

<h3 id="toc_1">Other tools I&rsquo;ve found</h3>

<p>I&rsquo;ve found a number of tools that are either not complete or don&rsquo;t quite
address the need, but reading the source code has been very productive.
There&rsquo;s <a href="http://www.perlmonks.org/?node_id=381053">Giuseppe Maxia&rsquo;s work in remote MySQL table
comparison</a>. I
based the MySQL Table Checksum tool on some of this work. Read the comments on
that link, and you&rsquo;ll see some follow-up from Fabien Coelho, who wrote
<a href="http://www.coelho.net/pg_comparator/">pg_comparator</a>. The
documentation for this tool is an excellent read, as it goes into great detail
on the algorithm used.</p>

<p>There are also a few projects that don&rsquo;t do what I&rsquo;m looking for.
<a href="http://www.mrjoy.com/datadiff/">datadiff</a> does a two-way
in-server comparison of two tables with <code>OUTER JOIN</code>, a fine technique but
inherently limited to two tables on one server, and not practical for
extremely large tables.
<a href="http://rossbeyer.net/software/mysql_coldiff/">coldiff</a> is
a more specialized variant of that tool. <a href="http:
//www.adamspiers.org/computing/mysqldiff/">mysqldiff</a> diffs the structure of two tables,
which I mention for completeness though it is not the problem I&rsquo;m trying to
solve.</p>

<h3 id="toc_2">The Maxia/Coelho bottom-up algorithm</h3>

<p>Without restating everything these smart people have written, here&rsquo;s a high-
level overview of the algorithm as presented by Maxia and Coelho:</p>

<ul>
<li>Compute a &ldquo;folding factor&rdquo; based on the number of rows in the table and/or user parameters.</li>
<li>Build successive levels of checksum tables bottom-up, starting at a row-level granularity and decreasing granularity by the &ldquo;folding factor&rdquo; with each level, until the final table has a single row.

<ul>
<li>Each row in the first table contains key column(s), a checksum of the key column(s), and a checksum of the whole row.</li>
<li>Each row in an intermediate-level summary table contains checksums for a group of rows in the next more granular level of summary data.</li>
<li>Groups are defined by taking checksums from the previous level modulo the folding factor.</li>
</ul></li>
<li>Beginning at the most aggregated level, walk the &ldquo;tree&rdquo; looking for the differences, honing in eventually to the offending rows.</li>
</ul>

<p>The &ldquo;folding factor&rdquo; is really a &ldquo;branching factor&rdquo; for the tree of summary
tables. If the factor is 128, each level in an intermediate summary table will
contain the groupwise checksum of about 128 rows in the next most granular
level summary table.</p>

<p>This algorithm has many strengths. For example, it uses a logarithmic search
to find rows that differ. It makes no assumptions about key distributions; the
modulo operation on the checksum should randomize the distribution of which
rows need to be fixed. It&rsquo;s also very generic, which means it works pretty
much the same on all tables. There&rsquo;s no need to think about the &ldquo;best way to
do it&rdquo; on a given table.</p>

<p>I am concerned about a few things, though. There&rsquo;s a lot of data in all these
summary tables. The first summary table contains as many rows as the table to
analyze. If I were to calculate and store these rows for a table with lots of
relatively narrow rows, I might be better off just copying the whole table
from one server to the other. Also, creating these tables is not replication-
friendly; the queries that run on the master will run on the slave too. This
might not be a problem for everyone, but it would not be acceptable for my
purposes.</p>

<p>The second part of the algorithm, walking the &ldquo;tree&rdquo; of summary tables to find
rows that differ, doesn&rsquo;t use any indexes in the implementations I&rsquo;ve seen.
Suppose I have a table with 128 million rows I want to analyze on two servers,
using a branching factor of 128 (the default). The first checksum table has
128 million rows; the second has 1 million, and so on. Repeated scans on these
tables will be inefficient, and given the randomization caused by the summary
checksums, will cause lots of random I/O. Indexes could be added on the
checksum modulo branching factor, but that&rsquo;s another column, <em>plus</em> an index
&ndash; this makes the table even bigger.</p>

<p>The checksum/modulo approach has another weakness. It defeats any
optimizations I might be able to make based on knowledge of where in the table
the rows differ. If the differences are grouped at the end of the table, for
example in an append-only table that just missed a few inserts on the slave,
the algorithm will distribute the &ldquo;pointers&rdquo; to these corrupt rows randomly
through the summary tables, even though the rows really live near each other.
Likewise, if my table contains client data and only one client is bad, the
same situation will happen. This is a major issue, especially in some large
tables I work with where we do things a client or account at a time. These and
other spatial and temporal locality scenarios are realistic, because lots of
real data is unevenly distributed. The checksum/modulo approach isn&rsquo;t optimal
for this.</p>

<p>Finally, the bottom-up approach doesn&rsquo;t allow for early optimization or
working in-memory. It builds the entire tree, then does the search. There&rsquo;s no
chance to &ldquo;prune&rdquo; the tree or try to keep a small working set. The flip side
of this is actually a strength: assuming that the whole tree needs to be
built, bottom-up is optimal. But most of my data isn&rsquo;t like that. If much of
the table is corrupt, I&rsquo;m going to do a <code>mysqldump</code> instead, so I want to
optimize for cases where I&rsquo;ll be able to prune the tree.</p>

<h3 id="toc_3">One solution: a top-down approach</h3>

<p>Given that I won&rsquo;t even be looking at a table unless the global checksum has
already found it differs, I am considering the following top-down approach, or
some variation thereof:</p>

<ul>
<li>Generate groupwise checksums for the whole table in a top-level grouping (more on that later).</li>
<li>If more than a certain fraction of the groups differ, quit. Too much of the table is different.</li>
<li>Otherwise descend depth-first into each group that has differences.</li>
</ul>

<p>I think this algorithm, with some tuning, will address most of my concerns
above. In particular, it will allow a smart DBA to specify how the grouping
and recursion should happen. The choice of grouping is actually the most
complicated part.</p>

<p>I&rsquo;d do this client-side, not server-side. I&rsquo;d <em>generate</em> the checksums server-
side, but then fetch them back to the client code and keep them in memory.
Given a good grouping, this shouldn&rsquo;t require much network traffic or memory
client-side, and will avoid locks, eliminate scratch tables, and keep the
queries from replicating.</p>

<p>In the best case, all other things being equal, it will require the server to
read about as many rows as the bottom-up approach, but it will exploit
locality &ndash; a client at a time, a day at a time, and so on. This is a huge
help, in my opinion; reducing random I/O is a high priority for me.</p>

<p>Given all this, I think top-down is better if there are not many changes to
resolve, or if they&rsquo;re grouped tightly together.</p>

<p>Some of the weaknesses I see are complexity, a proliferation of recursion and
grouping strategies, perhaps more network traffic, and susceptibility to edge
cases. Whereas the bottom-up approach has identical best and worst cases for
different distributions of corrupt rows (assuming the number of corrupt rows
is constant), the top-down approach suffers if there&rsquo;s no locality to exploit.
I&rsquo;m a bit worried about edge cases causing this to happen more than I think it
ought to.</p>

<p>Finally, and this could be either a strength or weakness, this approach lets
every level of the recursion have a different branching factor, which might be
appropriate or not &ndash; the DBA needs to decide.</p>

<h3 id="toc_4">Smart grouping and recursion</h3>

<p>I think the hardest part is choosing appropriate ways to group and &ldquo;drill
down&rdquo; into the table. Here are some possible strategies:</p>

<ul>
<li>Date groupings. We have a lot of data in InnoDB tables with day-first or week-first primary keys, which as you know creates a day-first or week-first clustered index. The first checksum I&rsquo;d run on these tables would be grouped by day.</li>
<li>Numeric groupings. Tables whose primary key is an auto-incremented number would probably be best grouped by division, for example, <code>floor(id/5000)</code> to group about 5000 neighboring rows together at a time.</li>
<li>Character groupings. If the primary key is a character string, I might group on the first few letters of the string.</li>
<li>Drill-down. Take for example one of our tables that is primary-keyed on IDs, which are auto-incremented numbers, and client account numbers. The best way to do the table I&rsquo;m thinking of is by account number, then numerically within that on ID. For the day-first table, I&rsquo;d group by day, then account number, and then by ID.</li>
<li>Exploit append-only tables. If a table is append-only, then corruption is likely in the most recent data, and I might try to examine only that part of the table. If there are updates and deletes to existing rows, this approach might not work.</li>
<li>Use defaults if the DBA doesn&rsquo;t specify anything. If there&rsquo;s a multi-column primary key, recurse one column at a time. If a single-column key, look for another key whose cardinality is less, and recurse from that to the primary key instead.</li>
</ul>

<p>I think the DBA will have to choose the best strategy on a table-by-table
basis, because I can&rsquo;t think of a good automatic way to do it. Even analyzing
the index structures on the table, and then trying to decide which are good
choices, is too risky to do automatically. For example, <code>SHOW INDEX</code> will show
estimated index cardinalities, but they&rsquo;re based on random dives into the
index tree and can be off by an order of magnitude or more.</p>

<h3 id="toc_5">How to resolve the differences</h3>

<p>Again assuming that this reconciliation is taking place between a master and
slave server, it&rsquo;s important to fix the rows without causing more trouble
while the fixing happens. For example, I don&rsquo;t want to do something that&rsquo;ll
propagate to another slave that&rsquo;s okay, and thereby mess it up, too.</p>

<p>Fixing the rows on the master, and letting the fixes propagate to the slave
via the normal means, might actually be a good idea. If a row doesn&rsquo;t exist or
is different on the slave, <code>REPLACE</code> or <code>INSERT .. ON DUPLICATE KEY UPDATE</code>
should fix the row on the slave without altering it on the master. If the row
exists on the slave but not the master, <code>DELETE</code> on the master should delete
it on the slave.</p>

<p>Peripheral benefits of this approach are that I don&rsquo;t need to set up an
account with write privileges on the slave. Also, if more than one slave has
troubles with the same rows, this should fix them all at the same time.</p>

<p>Issues I need to research are whether the different number of rows affected on
the slave will cause trouble, and if this can be solved with a temporary
<a href="http://dev.mysql.com/doc/refman/5.0/en
/replication-options.html">slave-skip-errors</a> setting. The manual may document this, but I can&rsquo;t
find it.</p>

<h3 id="toc_6">Next steps</h3>

<p>I&rsquo;m looking forward to your feedback, and then I plan to build a tool that&rsquo;ll
implement whatever algorithm emerges from that discussion. At this point,
assuming the above algorithm is as good as we can come up with together, I&rsquo;m
planning to actually implement both top-down and bottom-up approaches in the
tool, so the DBA can decide what to use. The tool will, like the rest of the
scripts in the <a href="http://sourceforge.net/projects/mysqltoolkit">MySQL
Toolkit</a>, be
command-line friendly (there are lots of proprietary &ldquo;visual tools&rdquo; to compare
and sync tables, but they don&rsquo;t interest me &ndash; plus, why would I ever trust
customer data to something I can&rsquo;t see source code for?). I also understand
that not everyone has the same narrowly-defined use case of re-syncing a
slave, so of course I&rsquo;ll make the tool more generic.</p>

<p>For my own use, ideally I&rsquo;ll be making sure the tool is rock-solid, then
defining rules for tables that frequently drift, and running a cron job to
automatically find which tables are different and fix them. If the <a href="http://sourceforge.net/projects/mysqltoolkit">MySQL
Table
Checksum</a>
tool finds a table is out of sync and I don&rsquo;t have a rule for it, it&rsquo;ll just
notify me and not try to fix it.</p>

<h3 id="toc_7">Summary</h3>

<p>In this article I proposed some ideas for a top-down, in-client, replication-
centric way to compare a table known to differ on a master and slave, find the
rows that differ, and resolve them. I&rsquo;m thinking about building a tool to
implement this algorithm, and would like your feedback on efficient ways to do
this.</p>

		                        <hr>
                        <h2>Comments</h2>
                        <div id="disqus_thread"></div>
                        <script type="text/javascript">
                            var disqus_shortname = 'xaprb';
                            (function() {
                                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                            })();
                        </script>

</div>
</div>

  </body>
</html>
